{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1875,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008,
      "grad_norm": 1.9326598644256592,
      "learning_rate": 2.6595744680851065e-06,
      "loss": 2.9612,
      "step": 5
    },
    {
      "epoch": 0.016,
      "grad_norm": 1.9305315017700195,
      "learning_rate": 5.319148936170213e-06,
      "loss": 2.9699,
      "step": 10
    },
    {
      "epoch": 0.024,
      "grad_norm": 2.021766185760498,
      "learning_rate": 7.97872340425532e-06,
      "loss": 3.0377,
      "step": 15
    },
    {
      "epoch": 0.032,
      "grad_norm": 1.7006874084472656,
      "learning_rate": 1.0638297872340426e-05,
      "loss": 2.7907,
      "step": 20
    },
    {
      "epoch": 0.04,
      "grad_norm": 1.5127403736114502,
      "learning_rate": 1.3297872340425532e-05,
      "loss": 2.857,
      "step": 25
    },
    {
      "epoch": 0.048,
      "grad_norm": 1.4117399454116821,
      "learning_rate": 1.595744680851064e-05,
      "loss": 2.7278,
      "step": 30
    },
    {
      "epoch": 0.056,
      "grad_norm": 1.345470666885376,
      "learning_rate": 1.8617021276595745e-05,
      "loss": 2.838,
      "step": 35
    },
    {
      "epoch": 0.064,
      "grad_norm": 1.4865217208862305,
      "learning_rate": 2.1276595744680852e-05,
      "loss": 2.8821,
      "step": 40
    },
    {
      "epoch": 0.072,
      "grad_norm": 1.319615364074707,
      "learning_rate": 2.393617021276596e-05,
      "loss": 2.7958,
      "step": 45
    },
    {
      "epoch": 0.08,
      "grad_norm": 1.3482152223587036,
      "learning_rate": 2.6595744680851064e-05,
      "loss": 2.6631,
      "step": 50
    },
    {
      "epoch": 0.088,
      "grad_norm": 1.3813902139663696,
      "learning_rate": 2.925531914893617e-05,
      "loss": 2.7272,
      "step": 55
    },
    {
      "epoch": 0.096,
      "grad_norm": 1.2193803787231445,
      "learning_rate": 3.191489361702128e-05,
      "loss": 2.585,
      "step": 60
    },
    {
      "epoch": 0.104,
      "grad_norm": 1.2313241958618164,
      "learning_rate": 3.4574468085106386e-05,
      "loss": 2.4475,
      "step": 65
    },
    {
      "epoch": 0.112,
      "grad_norm": 1.3972984552383423,
      "learning_rate": 3.723404255319149e-05,
      "loss": 2.4537,
      "step": 70
    },
    {
      "epoch": 0.12,
      "grad_norm": 1.579064965248108,
      "learning_rate": 3.9893617021276594e-05,
      "loss": 2.562,
      "step": 75
    },
    {
      "epoch": 0.128,
      "grad_norm": 1.3796718120574951,
      "learning_rate": 4.2553191489361704e-05,
      "loss": 2.5823,
      "step": 80
    },
    {
      "epoch": 0.136,
      "grad_norm": 1.798558235168457,
      "learning_rate": 4.5212765957446815e-05,
      "loss": 2.3375,
      "step": 85
    },
    {
      "epoch": 0.144,
      "grad_norm": 1.629202961921692,
      "learning_rate": 4.787234042553192e-05,
      "loss": 2.3678,
      "step": 90
    },
    {
      "epoch": 0.152,
      "grad_norm": 1.5067431926727295,
      "learning_rate": 5.053191489361703e-05,
      "loss": 2.4485,
      "step": 95
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.8535382747650146,
      "learning_rate": 5.319148936170213e-05,
      "loss": 2.3924,
      "step": 100
    },
    {
      "epoch": 0.168,
      "grad_norm": 1.5659713745117188,
      "learning_rate": 5.585106382978723e-05,
      "loss": 2.2866,
      "step": 105
    },
    {
      "epoch": 0.176,
      "grad_norm": 1.5942720174789429,
      "learning_rate": 5.851063829787234e-05,
      "loss": 2.3085,
      "step": 110
    },
    {
      "epoch": 0.184,
      "grad_norm": 1.678712248802185,
      "learning_rate": 6.117021276595745e-05,
      "loss": 2.3825,
      "step": 115
    },
    {
      "epoch": 0.192,
      "grad_norm": 1.7821645736694336,
      "learning_rate": 6.382978723404256e-05,
      "loss": 2.3051,
      "step": 120
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.8147886991500854,
      "learning_rate": 6.648936170212766e-05,
      "loss": 2.329,
      "step": 125
    },
    {
      "epoch": 0.208,
      "grad_norm": 1.7026976346969604,
      "learning_rate": 6.914893617021277e-05,
      "loss": 2.2491,
      "step": 130
    },
    {
      "epoch": 0.216,
      "grad_norm": 2.2013323307037354,
      "learning_rate": 7.180851063829788e-05,
      "loss": 2.3122,
      "step": 135
    },
    {
      "epoch": 0.224,
      "grad_norm": 1.8865224123001099,
      "learning_rate": 7.446808510638298e-05,
      "loss": 2.2053,
      "step": 140
    },
    {
      "epoch": 0.232,
      "grad_norm": 2.296703815460205,
      "learning_rate": 7.712765957446809e-05,
      "loss": 2.2623,
      "step": 145
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.9041025638580322,
      "learning_rate": 7.978723404255319e-05,
      "loss": 2.1909,
      "step": 150
    },
    {
      "epoch": 0.248,
      "grad_norm": 1.9812685251235962,
      "learning_rate": 8.244680851063831e-05,
      "loss": 2.2467,
      "step": 155
    },
    {
      "epoch": 0.256,
      "grad_norm": 2.3412952423095703,
      "learning_rate": 8.510638297872341e-05,
      "loss": 2.2212,
      "step": 160
    },
    {
      "epoch": 0.264,
      "grad_norm": 2.036829710006714,
      "learning_rate": 8.77659574468085e-05,
      "loss": 2.2195,
      "step": 165
    },
    {
      "epoch": 0.272,
      "grad_norm": 2.1550610065460205,
      "learning_rate": 9.042553191489363e-05,
      "loss": 2.1707,
      "step": 170
    },
    {
      "epoch": 0.28,
      "grad_norm": 1.969224452972412,
      "learning_rate": 9.308510638297873e-05,
      "loss": 2.0697,
      "step": 175
    },
    {
      "epoch": 0.288,
      "grad_norm": 2.075442314147949,
      "learning_rate": 9.574468085106384e-05,
      "loss": 2.1553,
      "step": 180
    },
    {
      "epoch": 0.296,
      "grad_norm": 2.1400833129882812,
      "learning_rate": 9.840425531914894e-05,
      "loss": 2.1325,
      "step": 185
    },
    {
      "epoch": 0.304,
      "grad_norm": 2.1886560916900635,
      "learning_rate": 9.999965320799376e-05,
      "loss": 2.2387,
      "step": 190
    },
    {
      "epoch": 0.312,
      "grad_norm": 2.3736352920532227,
      "learning_rate": 9.999575185316994e-05,
      "loss": 2.1216,
      "step": 195
    },
    {
      "epoch": 0.32,
      "grad_norm": 2.8226940631866455,
      "learning_rate": 9.998751599287959e-05,
      "loss": 2.2094,
      "step": 200
    },
    {
      "epoch": 0.328,
      "grad_norm": 2.674506425857544,
      "learning_rate": 9.9974946341151e-05,
      "loss": 2.0937,
      "step": 205
    },
    {
      "epoch": 0.336,
      "grad_norm": 2.3042075634002686,
      "learning_rate": 9.995804398774127e-05,
      "loss": 2.1093,
      "step": 210
    },
    {
      "epoch": 0.344,
      "grad_norm": 2.4525868892669678,
      "learning_rate": 9.993681039804175e-05,
      "loss": 2.0202,
      "step": 215
    },
    {
      "epoch": 0.352,
      "grad_norm": 2.2029025554656982,
      "learning_rate": 9.991124741295106e-05,
      "loss": 2.1614,
      "step": 220
    },
    {
      "epoch": 0.36,
      "grad_norm": 2.5879688262939453,
      "learning_rate": 9.988135724871546e-05,
      "loss": 2.1476,
      "step": 225
    },
    {
      "epoch": 0.368,
      "grad_norm": 2.156104564666748,
      "learning_rate": 9.984714249673675e-05,
      "loss": 2.0344,
      "step": 230
    },
    {
      "epoch": 0.376,
      "grad_norm": 2.2839272022247314,
      "learning_rate": 9.980860612334752e-05,
      "loss": 2.05,
      "step": 235
    },
    {
      "epoch": 0.384,
      "grad_norm": 2.258718729019165,
      "learning_rate": 9.97657514695541e-05,
      "loss": 2.0185,
      "step": 240
    },
    {
      "epoch": 0.392,
      "grad_norm": 2.7004623413085938,
      "learning_rate": 9.971858225074673e-05,
      "loss": 2.0007,
      "step": 245
    },
    {
      "epoch": 0.4,
      "grad_norm": 2.4340596199035645,
      "learning_rate": 9.966710255637764e-05,
      "loss": 2.1364,
      "step": 250
    },
    {
      "epoch": 0.408,
      "grad_norm": 2.1263246536254883,
      "learning_rate": 9.961131684960635e-05,
      "loss": 2.0419,
      "step": 255
    },
    {
      "epoch": 0.416,
      "grad_norm": 2.507828712463379,
      "learning_rate": 9.955122996691278e-05,
      "loss": 1.97,
      "step": 260
    },
    {
      "epoch": 0.424,
      "grad_norm": 2.969642162322998,
      "learning_rate": 9.9486847117678e-05,
      "loss": 1.9908,
      "step": 265
    },
    {
      "epoch": 0.432,
      "grad_norm": 2.5745460987091064,
      "learning_rate": 9.941817388373247e-05,
      "loss": 2.0715,
      "step": 270
    },
    {
      "epoch": 0.44,
      "grad_norm": 2.8534510135650635,
      "learning_rate": 9.934521621887223e-05,
      "loss": 2.0293,
      "step": 275
    },
    {
      "epoch": 0.448,
      "grad_norm": 2.5516414642333984,
      "learning_rate": 9.92679804483426e-05,
      "loss": 2.0557,
      "step": 280
    },
    {
      "epoch": 0.456,
      "grad_norm": 2.7928881645202637,
      "learning_rate": 9.918647326828992e-05,
      "loss": 2.1157,
      "step": 285
    },
    {
      "epoch": 0.464,
      "grad_norm": 2.7597694396972656,
      "learning_rate": 9.910070174518092e-05,
      "loss": 2.0173,
      "step": 290
    },
    {
      "epoch": 0.472,
      "grad_norm": 2.475790500640869,
      "learning_rate": 9.901067331519012e-05,
      "loss": 2.0759,
      "step": 295
    },
    {
      "epoch": 0.48,
      "grad_norm": 2.524580955505371,
      "learning_rate": 9.891639578355511e-05,
      "loss": 2.044,
      "step": 300
    },
    {
      "epoch": 0.488,
      "grad_norm": 2.5166616439819336,
      "learning_rate": 9.881787732389986e-05,
      "loss": 1.9293,
      "step": 305
    },
    {
      "epoch": 0.496,
      "grad_norm": 2.3100948333740234,
      "learning_rate": 9.871512647752612e-05,
      "loss": 2.0005,
      "step": 310
    },
    {
      "epoch": 0.504,
      "grad_norm": 2.7276461124420166,
      "learning_rate": 9.860815215267287e-05,
      "loss": 1.9787,
      "step": 315
    },
    {
      "epoch": 0.512,
      "grad_norm": 2.7206456661224365,
      "learning_rate": 9.849696362374399e-05,
      "loss": 1.9529,
      "step": 320
    },
    {
      "epoch": 0.52,
      "grad_norm": 2.452375650405884,
      "learning_rate": 9.838157053050423e-05,
      "loss": 1.9882,
      "step": 325
    },
    {
      "epoch": 0.528,
      "grad_norm": 2.5117905139923096,
      "learning_rate": 9.826198287724346e-05,
      "loss": 1.9646,
      "step": 330
    },
    {
      "epoch": 0.536,
      "grad_norm": 2.732433795928955,
      "learning_rate": 9.813821103190932e-05,
      "loss": 2.0596,
      "step": 335
    },
    {
      "epoch": 0.544,
      "grad_norm": 2.5430614948272705,
      "learning_rate": 9.801026572520832e-05,
      "loss": 2.0269,
      "step": 340
    },
    {
      "epoch": 0.552,
      "grad_norm": 2.6096878051757812,
      "learning_rate": 9.787815804967553e-05,
      "loss": 1.9859,
      "step": 345
    },
    {
      "epoch": 0.56,
      "grad_norm": 2.4001224040985107,
      "learning_rate": 9.77418994587129e-05,
      "loss": 1.9644,
      "step": 350
    },
    {
      "epoch": 0.568,
      "grad_norm": 2.644193410873413,
      "learning_rate": 9.760150176559627e-05,
      "loss": 2.0374,
      "step": 355
    },
    {
      "epoch": 0.576,
      "grad_norm": 2.8117356300354004,
      "learning_rate": 9.745697714245119e-05,
      "loss": 1.9324,
      "step": 360
    },
    {
      "epoch": 0.584,
      "grad_norm": 2.577946186065674,
      "learning_rate": 9.730833811919762e-05,
      "loss": 1.915,
      "step": 365
    },
    {
      "epoch": 0.592,
      "grad_norm": 2.7268130779266357,
      "learning_rate": 9.715559758246363e-05,
      "loss": 2.0706,
      "step": 370
    },
    {
      "epoch": 0.6,
      "grad_norm": 2.4221978187561035,
      "learning_rate": 9.699876877446814e-05,
      "loss": 1.946,
      "step": 375
    },
    {
      "epoch": 0.608,
      "grad_norm": 2.58005428314209,
      "learning_rate": 9.683786529187287e-05,
      "loss": 2.0625,
      "step": 380
    },
    {
      "epoch": 0.616,
      "grad_norm": 2.5389621257781982,
      "learning_rate": 9.667290108460354e-05,
      "loss": 2.0473,
      "step": 385
    },
    {
      "epoch": 0.624,
      "grad_norm": 2.6681466102600098,
      "learning_rate": 9.650389045464045e-05,
      "loss": 1.9924,
      "step": 390
    },
    {
      "epoch": 0.632,
      "grad_norm": 2.6876394748687744,
      "learning_rate": 9.633084805477856e-05,
      "loss": 1.9313,
      "step": 395
    },
    {
      "epoch": 0.64,
      "grad_norm": 2.848313808441162,
      "learning_rate": 9.615378888735705e-05,
      "loss": 1.8888,
      "step": 400
    },
    {
      "epoch": 0.648,
      "grad_norm": 2.7674622535705566,
      "learning_rate": 9.597272830295876e-05,
      "loss": 1.9342,
      "step": 405
    },
    {
      "epoch": 0.656,
      "grad_norm": 2.760688543319702,
      "learning_rate": 9.578768199907921e-05,
      "loss": 1.9685,
      "step": 410
    },
    {
      "epoch": 0.664,
      "grad_norm": 2.4853439331054688,
      "learning_rate": 9.55986660187658e-05,
      "loss": 1.9574,
      "step": 415
    },
    {
      "epoch": 0.672,
      "grad_norm": 2.6972451210021973,
      "learning_rate": 9.540569674922685e-05,
      "loss": 1.9186,
      "step": 420
    },
    {
      "epoch": 0.68,
      "grad_norm": 2.5829458236694336,
      "learning_rate": 9.520879092041084e-05,
      "loss": 1.9512,
      "step": 425
    },
    {
      "epoch": 0.688,
      "grad_norm": 2.3844668865203857,
      "learning_rate": 9.500796560355602e-05,
      "loss": 1.9557,
      "step": 430
    },
    {
      "epoch": 0.696,
      "grad_norm": 2.658639430999756,
      "learning_rate": 9.480323820971037e-05,
      "loss": 2.0098,
      "step": 435
    },
    {
      "epoch": 0.704,
      "grad_norm": 2.3635051250457764,
      "learning_rate": 9.459462648822208e-05,
      "loss": 1.9498,
      "step": 440
    },
    {
      "epoch": 0.712,
      "grad_norm": 2.6514716148376465,
      "learning_rate": 9.438214852520073e-05,
      "loss": 2.0135,
      "step": 445
    },
    {
      "epoch": 0.72,
      "grad_norm": 2.6095807552337646,
      "learning_rate": 9.41658227419493e-05,
      "loss": 1.9145,
      "step": 450
    },
    {
      "epoch": 0.728,
      "grad_norm": 2.662898302078247,
      "learning_rate": 9.394566789336707e-05,
      "loss": 1.9476,
      "step": 455
    },
    {
      "epoch": 0.736,
      "grad_norm": 2.341890335083008,
      "learning_rate": 9.372170306632359e-05,
      "loss": 1.8853,
      "step": 460
    },
    {
      "epoch": 0.744,
      "grad_norm": 2.974558115005493,
      "learning_rate": 9.349394767800396e-05,
      "loss": 1.9349,
      "step": 465
    },
    {
      "epoch": 0.752,
      "grad_norm": 2.806424856185913,
      "learning_rate": 9.326242147422537e-05,
      "loss": 1.9565,
      "step": 470
    },
    {
      "epoch": 0.76,
      "grad_norm": 2.61542010307312,
      "learning_rate": 9.302714452772516e-05,
      "loss": 1.9459,
      "step": 475
    },
    {
      "epoch": 0.768,
      "grad_norm": 2.5624935626983643,
      "learning_rate": 9.278813723642059e-05,
      "loss": 2.0092,
      "step": 480
    },
    {
      "epoch": 0.776,
      "grad_norm": 2.4377779960632324,
      "learning_rate": 9.254542032164046e-05,
      "loss": 1.8696,
      "step": 485
    },
    {
      "epoch": 0.784,
      "grad_norm": 2.4273641109466553,
      "learning_rate": 9.22990148263285e-05,
      "loss": 1.852,
      "step": 490
    },
    {
      "epoch": 0.792,
      "grad_norm": 2.6253695487976074,
      "learning_rate": 9.204894211321906e-05,
      "loss": 1.9956,
      "step": 495
    },
    {
      "epoch": 0.8,
      "grad_norm": 2.565995693206787,
      "learning_rate": 9.179522386298507e-05,
      "loss": 1.8741,
      "step": 500
    },
    {
      "epoch": 0.808,
      "grad_norm": 2.668694496154785,
      "learning_rate": 9.153788207235827e-05,
      "loss": 1.9613,
      "step": 505
    },
    {
      "epoch": 0.816,
      "grad_norm": 2.578204393386841,
      "learning_rate": 9.127693905222224e-05,
      "loss": 1.8817,
      "step": 510
    },
    {
      "epoch": 0.824,
      "grad_norm": 2.4611613750457764,
      "learning_rate": 9.101241742567801e-05,
      "loss": 2.0149,
      "step": 515
    },
    {
      "epoch": 0.832,
      "grad_norm": 2.541486978530884,
      "learning_rate": 9.074434012608281e-05,
      "loss": 1.8848,
      "step": 520
    },
    {
      "epoch": 0.84,
      "grad_norm": 2.4503657817840576,
      "learning_rate": 9.047273039506174e-05,
      "loss": 1.8091,
      "step": 525
    },
    {
      "epoch": 0.848,
      "grad_norm": 2.3917922973632812,
      "learning_rate": 9.019761178049279e-05,
      "loss": 1.8204,
      "step": 530
    },
    {
      "epoch": 0.856,
      "grad_norm": 3.051483631134033,
      "learning_rate": 8.991900813446523e-05,
      "loss": 1.9669,
      "step": 535
    },
    {
      "epoch": 0.864,
      "grad_norm": 2.6042590141296387,
      "learning_rate": 8.963694361121185e-05,
      "loss": 1.9457,
      "step": 540
    },
    {
      "epoch": 0.872,
      "grad_norm": 2.3669168949127197,
      "learning_rate": 8.935144266501469e-05,
      "loss": 1.8317,
      "step": 545
    },
    {
      "epoch": 0.88,
      "grad_norm": 2.610785722732544,
      "learning_rate": 8.906253004808505e-05,
      "loss": 1.9038,
      "step": 550
    },
    {
      "epoch": 0.888,
      "grad_norm": 2.9440250396728516,
      "learning_rate": 8.877023080841737e-05,
      "loss": 1.9097,
      "step": 555
    },
    {
      "epoch": 0.896,
      "grad_norm": 2.6702845096588135,
      "learning_rate": 8.847457028761784e-05,
      "loss": 1.8935,
      "step": 560
    },
    {
      "epoch": 0.904,
      "grad_norm": 2.7512378692626953,
      "learning_rate": 8.817557411870717e-05,
      "loss": 1.7638,
      "step": 565
    },
    {
      "epoch": 0.912,
      "grad_norm": 2.763566732406616,
      "learning_rate": 8.787326822389835e-05,
      "loss": 1.8974,
      "step": 570
    },
    {
      "epoch": 0.92,
      "grad_norm": 2.5151736736297607,
      "learning_rate": 8.756767881234929e-05,
      "loss": 1.8737,
      "step": 575
    },
    {
      "epoch": 0.928,
      "grad_norm": 3.0033206939697266,
      "learning_rate": 8.725883237789045e-05,
      "loss": 1.9207,
      "step": 580
    },
    {
      "epoch": 0.936,
      "grad_norm": 2.861266613006592,
      "learning_rate": 8.6946755696728e-05,
      "loss": 1.8762,
      "step": 585
    },
    {
      "epoch": 0.944,
      "grad_norm": 2.6729025840759277,
      "learning_rate": 8.663147582512232e-05,
      "loss": 1.8967,
      "step": 590
    },
    {
      "epoch": 0.952,
      "grad_norm": 3.1385326385498047,
      "learning_rate": 8.631302009704234e-05,
      "loss": 1.9049,
      "step": 595
    },
    {
      "epoch": 0.96,
      "grad_norm": 2.6730992794036865,
      "learning_rate": 8.599141612179571e-05,
      "loss": 1.9678,
      "step": 600
    },
    {
      "epoch": 0.968,
      "grad_norm": 2.714080810546875,
      "learning_rate": 8.566669178163514e-05,
      "loss": 1.9383,
      "step": 605
    },
    {
      "epoch": 0.976,
      "grad_norm": 2.5152103900909424,
      "learning_rate": 8.533887522934114e-05,
      "loss": 1.8073,
      "step": 610
    },
    {
      "epoch": 0.984,
      "grad_norm": 2.6261556148529053,
      "learning_rate": 8.50079948857812e-05,
      "loss": 1.8834,
      "step": 615
    },
    {
      "epoch": 0.992,
      "grad_norm": 2.383434772491455,
      "learning_rate": 8.467407943744574e-05,
      "loss": 1.8554,
      "step": 620
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.8996570110321045,
      "learning_rate": 8.433715783396114e-05,
      "loss": 1.9213,
      "step": 625
    },
    {
      "epoch": 1.008,
      "grad_norm": 2.6921005249023438,
      "learning_rate": 8.399725928557985e-05,
      "loss": 1.9151,
      "step": 630
    },
    {
      "epoch": 1.016,
      "grad_norm": 2.759444236755371,
      "learning_rate": 8.365441326064789e-05,
      "loss": 1.8708,
      "step": 635
    },
    {
      "epoch": 1.024,
      "grad_norm": 2.799769163131714,
      "learning_rate": 8.330864948305009e-05,
      "loss": 1.7868,
      "step": 640
    },
    {
      "epoch": 1.032,
      "grad_norm": 2.751934051513672,
      "learning_rate": 8.2959997929633e-05,
      "loss": 1.8607,
      "step": 645
    },
    {
      "epoch": 1.04,
      "grad_norm": 2.692047357559204,
      "learning_rate": 8.260848882760615e-05,
      "loss": 1.7644,
      "step": 650
    },
    {
      "epoch": 1.048,
      "grad_norm": 2.7481000423431396,
      "learning_rate": 8.225415265192127e-05,
      "loss": 1.893,
      "step": 655
    },
    {
      "epoch": 1.056,
      "grad_norm": 2.79176926612854,
      "learning_rate": 8.189702012263021e-05,
      "loss": 1.7522,
      "step": 660
    },
    {
      "epoch": 1.064,
      "grad_norm": 2.5285933017730713,
      "learning_rate": 8.153712220222164e-05,
      "loss": 1.8176,
      "step": 665
    },
    {
      "epoch": 1.072,
      "grad_norm": 2.482865810394287,
      "learning_rate": 8.117449009293668e-05,
      "loss": 1.8212,
      "step": 670
    },
    {
      "epoch": 1.08,
      "grad_norm": 2.4606707096099854,
      "learning_rate": 8.08091552340637e-05,
      "loss": 1.8745,
      "step": 675
    },
    {
      "epoch": 1.088,
      "grad_norm": 2.61252760887146,
      "learning_rate": 8.044114929921264e-05,
      "loss": 1.8021,
      "step": 680
    },
    {
      "epoch": 1.096,
      "grad_norm": 2.7807435989379883,
      "learning_rate": 8.007050419356899e-05,
      "loss": 1.8734,
      "step": 685
    },
    {
      "epoch": 1.104,
      "grad_norm": 2.6877248287200928,
      "learning_rate": 7.969725205112766e-05,
      "loss": 1.8304,
      "step": 690
    },
    {
      "epoch": 1.112,
      "grad_norm": 2.8287460803985596,
      "learning_rate": 7.932142523190711e-05,
      "loss": 1.7821,
      "step": 695
    },
    {
      "epoch": 1.12,
      "grad_norm": 3.0511820316314697,
      "learning_rate": 7.894305631914373e-05,
      "loss": 1.7292,
      "step": 700
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 2.8164875507354736,
      "learning_rate": 7.856217811646706e-05,
      "loss": 1.7857,
      "step": 705
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 2.833160400390625,
      "learning_rate": 7.817882364505569e-05,
      "loss": 1.9017,
      "step": 710
    },
    {
      "epoch": 1.144,
      "grad_norm": 2.5269887447357178,
      "learning_rate": 7.779302614077449e-05,
      "loss": 1.7127,
      "step": 715
    },
    {
      "epoch": 1.152,
      "grad_norm": 2.9898765087127686,
      "learning_rate": 7.740481905129306e-05,
      "loss": 1.7389,
      "step": 720
    },
    {
      "epoch": 1.16,
      "grad_norm": 2.6136374473571777,
      "learning_rate": 7.701423603318604e-05,
      "loss": 1.7699,
      "step": 725
    },
    {
      "epoch": 1.168,
      "grad_norm": 2.860957384109497,
      "learning_rate": 7.662131094901499e-05,
      "loss": 1.8334,
      "step": 730
    },
    {
      "epoch": 1.176,
      "grad_norm": 3.1888012886047363,
      "learning_rate": 7.622607786439279e-05,
      "loss": 1.8631,
      "step": 735
    },
    {
      "epoch": 1.184,
      "grad_norm": 2.5845038890838623,
      "learning_rate": 7.582857104503001e-05,
      "loss": 1.7539,
      "step": 740
    },
    {
      "epoch": 1.192,
      "grad_norm": 2.9709627628326416,
      "learning_rate": 7.542882495376437e-05,
      "loss": 1.7776,
      "step": 745
    },
    {
      "epoch": 1.2,
      "grad_norm": 2.5712544918060303,
      "learning_rate": 7.502687424757278e-05,
      "loss": 1.7572,
      "step": 750
    },
    {
      "epoch": 1.208,
      "grad_norm": 2.886047124862671,
      "learning_rate": 7.46227537745667e-05,
      "loss": 1.8631,
      "step": 755
    },
    {
      "epoch": 1.216,
      "grad_norm": 2.738414764404297,
      "learning_rate": 7.421649857097092e-05,
      "loss": 1.8531,
      "step": 760
    },
    {
      "epoch": 1.224,
      "grad_norm": 2.8889832496643066,
      "learning_rate": 7.380814385808595e-05,
      "loss": 1.8506,
      "step": 765
    },
    {
      "epoch": 1.232,
      "grad_norm": 2.714817523956299,
      "learning_rate": 7.339772503923445e-05,
      "loss": 1.7926,
      "step": 770
    },
    {
      "epoch": 1.24,
      "grad_norm": 2.877664804458618,
      "learning_rate": 7.298527769669187e-05,
      "loss": 1.8645,
      "step": 775
    },
    {
      "epoch": 1.248,
      "grad_norm": 2.561452865600586,
      "learning_rate": 7.257083758860158e-05,
      "loss": 1.9069,
      "step": 780
    },
    {
      "epoch": 1.256,
      "grad_norm": 2.709618091583252,
      "learning_rate": 7.215444064587462e-05,
      "loss": 1.767,
      "step": 785
    },
    {
      "epoch": 1.264,
      "grad_norm": 2.782973289489746,
      "learning_rate": 7.173612296907472e-05,
      "loss": 1.7994,
      "step": 790
    },
    {
      "epoch": 1.272,
      "grad_norm": 2.999854326248169,
      "learning_rate": 7.131592082528837e-05,
      "loss": 1.7908,
      "step": 795
    },
    {
      "epoch": 1.28,
      "grad_norm": 2.5481157302856445,
      "learning_rate": 7.089387064498056e-05,
      "loss": 1.8107,
      "step": 800
    },
    {
      "epoch": 1.288,
      "grad_norm": 2.812736749649048,
      "learning_rate": 7.047000901883645e-05,
      "loss": 1.7552,
      "step": 805
    },
    {
      "epoch": 1.296,
      "grad_norm": 2.7314603328704834,
      "learning_rate": 7.004437269458894e-05,
      "loss": 1.7612,
      "step": 810
    },
    {
      "epoch": 1.304,
      "grad_norm": 2.4629745483398438,
      "learning_rate": 6.961699857383279e-05,
      "loss": 1.7845,
      "step": 815
    },
    {
      "epoch": 1.312,
      "grad_norm": 2.9338059425354004,
      "learning_rate": 6.91879237088253e-05,
      "loss": 1.9142,
      "step": 820
    },
    {
      "epoch": 1.32,
      "grad_norm": 3.3893024921417236,
      "learning_rate": 6.875718529927405e-05,
      "loss": 1.7979,
      "step": 825
    },
    {
      "epoch": 1.328,
      "grad_norm": 3.395961046218872,
      "learning_rate": 6.832482068911167e-05,
      "loss": 1.7419,
      "step": 830
    },
    {
      "epoch": 1.336,
      "grad_norm": 2.6271793842315674,
      "learning_rate": 6.789086736325834e-05,
      "loss": 1.744,
      "step": 835
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 3.0961148738861084,
      "learning_rate": 6.745536294437187e-05,
      "loss": 1.7934,
      "step": 840
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 2.9472146034240723,
      "learning_rate": 6.701834518958587e-05,
      "loss": 1.8133,
      "step": 845
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 2.8017752170562744,
      "learning_rate": 6.657985198723643e-05,
      "loss": 1.7786,
      "step": 850
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 2.6301047801971436,
      "learning_rate": 6.613992135357713e-05,
      "loss": 1.6785,
      "step": 855
    },
    {
      "epoch": 1.376,
      "grad_norm": 2.5452427864074707,
      "learning_rate": 6.569859142948328e-05,
      "loss": 1.7794,
      "step": 860
    },
    {
      "epoch": 1.384,
      "grad_norm": 2.898697853088379,
      "learning_rate": 6.52559004771451e-05,
      "loss": 1.8216,
      "step": 865
    },
    {
      "epoch": 1.392,
      "grad_norm": 2.8243870735168457,
      "learning_rate": 6.481188687675057e-05,
      "loss": 1.8208,
      "step": 870
    },
    {
      "epoch": 1.4,
      "grad_norm": 2.918637275695801,
      "learning_rate": 6.436658912315788e-05,
      "loss": 1.7744,
      "step": 875
    },
    {
      "epoch": 1.408,
      "grad_norm": 2.959973096847534,
      "learning_rate": 6.392004582255807e-05,
      "loss": 1.7646,
      "step": 880
    },
    {
      "epoch": 1.416,
      "grad_norm": 3.194033622741699,
      "learning_rate": 6.347229568912795e-05,
      "loss": 1.8982,
      "step": 885
    },
    {
      "epoch": 1.424,
      "grad_norm": 2.8379557132720947,
      "learning_rate": 6.302337754167369e-05,
      "loss": 1.851,
      "step": 890
    },
    {
      "epoch": 1.432,
      "grad_norm": 2.817925214767456,
      "learning_rate": 6.257333030026538e-05,
      "loss": 1.83,
      "step": 895
    },
    {
      "epoch": 1.44,
      "grad_norm": 2.795621633529663,
      "learning_rate": 6.212219298286261e-05,
      "loss": 1.7863,
      "step": 900
    },
    {
      "epoch": 1.448,
      "grad_norm": 3.1003854274749756,
      "learning_rate": 6.167000470193189e-05,
      "loss": 1.7654,
      "step": 905
    },
    {
      "epoch": 1.456,
      "grad_norm": 3.1503663063049316,
      "learning_rate": 6.121680466105559e-05,
      "loss": 1.773,
      "step": 910
    },
    {
      "epoch": 1.464,
      "grad_norm": 2.7271785736083984,
      "learning_rate": 6.076263215153307e-05,
      "loss": 1.8338,
      "step": 915
    },
    {
      "epoch": 1.472,
      "grad_norm": 2.883152484893799,
      "learning_rate": 6.030752654897435e-05,
      "loss": 1.8105,
      "step": 920
    },
    {
      "epoch": 1.48,
      "grad_norm": 2.848935127258301,
      "learning_rate": 5.9851527309886165e-05,
      "loss": 1.7587,
      "step": 925
    },
    {
      "epoch": 1.488,
      "grad_norm": 2.644052028656006,
      "learning_rate": 5.939467396825137e-05,
      "loss": 1.7107,
      "step": 930
    },
    {
      "epoch": 1.496,
      "grad_norm": 3.4633476734161377,
      "learning_rate": 5.893700613210128e-05,
      "loss": 1.8229,
      "step": 935
    },
    {
      "epoch": 1.504,
      "grad_norm": 3.2174384593963623,
      "learning_rate": 5.847856348008188e-05,
      "loss": 1.8162,
      "step": 940
    },
    {
      "epoch": 1.512,
      "grad_norm": 2.7070343494415283,
      "learning_rate": 5.801938575801372e-05,
      "loss": 1.71,
      "step": 945
    },
    {
      "epoch": 1.52,
      "grad_norm": 2.8284332752227783,
      "learning_rate": 5.755951277544607e-05,
      "loss": 1.7951,
      "step": 950
    },
    {
      "epoch": 1.528,
      "grad_norm": 2.999467134475708,
      "learning_rate": 5.709898440220551e-05,
      "loss": 1.8514,
      "step": 955
    },
    {
      "epoch": 1.536,
      "grad_norm": 2.859790802001953,
      "learning_rate": 5.663784056493936e-05,
      "loss": 1.8148,
      "step": 960
    },
    {
      "epoch": 1.544,
      "grad_norm": 2.744636058807373,
      "learning_rate": 5.61761212436541e-05,
      "loss": 1.7105,
      "step": 965
    },
    {
      "epoch": 1.552,
      "grad_norm": 2.824763774871826,
      "learning_rate": 5.5713866468249235e-05,
      "loss": 1.8346,
      "step": 970
    },
    {
      "epoch": 1.56,
      "grad_norm": 3.004284143447876,
      "learning_rate": 5.5251116315046784e-05,
      "loss": 1.6984,
      "step": 975
    },
    {
      "epoch": 1.568,
      "grad_norm": 2.8613734245300293,
      "learning_rate": 5.4787910903316774e-05,
      "loss": 1.7597,
      "step": 980
    },
    {
      "epoch": 1.576,
      "grad_norm": 2.804500102996826,
      "learning_rate": 5.4324290391798997e-05,
      "loss": 1.8307,
      "step": 985
    },
    {
      "epoch": 1.584,
      "grad_norm": 2.818439483642578,
      "learning_rate": 5.3860294975221335e-05,
      "loss": 1.8143,
      "step": 990
    },
    {
      "epoch": 1.592,
      "grad_norm": 3.0506930351257324,
      "learning_rate": 5.339596488081501e-05,
      "loss": 1.8548,
      "step": 995
    },
    {
      "epoch": 1.6,
      "grad_norm": 2.9967496395111084,
      "learning_rate": 5.293134036482698e-05,
      "loss": 1.7634,
      "step": 1000
    },
    {
      "epoch": 1.608,
      "grad_norm": 2.8717753887176514,
      "learning_rate": 5.2466461709029755e-05,
      "loss": 1.6622,
      "step": 1005
    },
    {
      "epoch": 1.616,
      "grad_norm": 2.8373863697052,
      "learning_rate": 5.200136921722919e-05,
      "loss": 1.7895,
      "step": 1010
    },
    {
      "epoch": 1.624,
      "grad_norm": 2.8020145893096924,
      "learning_rate": 5.153610321177014e-05,
      "loss": 1.7171,
      "step": 1015
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 3.028665542602539,
      "learning_rate": 5.107070403004067e-05,
      "loss": 1.7585,
      "step": 1020
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 2.7440474033355713,
      "learning_rate": 5.06052120209749e-05,
      "loss": 1.8016,
      "step": 1025
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 3.1364736557006836,
      "learning_rate": 5.0139667541554825e-05,
      "loss": 1.7241,
      "step": 1030
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 2.9704999923706055,
      "learning_rate": 4.967411095331149e-05,
      "loss": 1.7492,
      "step": 1035
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 3.0574049949645996,
      "learning_rate": 4.920858261882578e-05,
      "loss": 1.79,
      "step": 1040
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 2.809690475463867,
      "learning_rate": 4.8743122898229e-05,
      "loss": 1.7705,
      "step": 1045
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 2.6708106994628906,
      "learning_rate": 4.827777214570384e-05,
      "loss": 1.6721,
      "step": 1050
    },
    {
      "epoch": 1.688,
      "grad_norm": 3.029660940170288,
      "learning_rate": 4.781257070598571e-05,
      "loss": 1.7253,
      "step": 1055
    },
    {
      "epoch": 1.696,
      "grad_norm": 2.999544858932495,
      "learning_rate": 4.734755891086498e-05,
      "loss": 1.8555,
      "step": 1060
    },
    {
      "epoch": 1.704,
      "grad_norm": 2.9827592372894287,
      "learning_rate": 4.688277707569035e-05,
      "loss": 1.8633,
      "step": 1065
    },
    {
      "epoch": 1.712,
      "grad_norm": 2.543870687484741,
      "learning_rate": 4.641826549587352e-05,
      "loss": 1.7326,
      "step": 1070
    },
    {
      "epoch": 1.72,
      "grad_norm": 2.6721267700195312,
      "learning_rate": 4.5954064443395763e-05,
      "loss": 1.6971,
      "step": 1075
    },
    {
      "epoch": 1.728,
      "grad_norm": 2.9523534774780273,
      "learning_rate": 4.54902141633164e-05,
      "loss": 1.6987,
      "step": 1080
    },
    {
      "epoch": 1.736,
      "grad_norm": 3.318876266479492,
      "learning_rate": 4.502675487028369e-05,
      "loss": 1.7278,
      "step": 1085
    },
    {
      "epoch": 1.744,
      "grad_norm": 2.732755184173584,
      "learning_rate": 4.456372674504828e-05,
      "loss": 1.7074,
      "step": 1090
    },
    {
      "epoch": 1.752,
      "grad_norm": 2.8572866916656494,
      "learning_rate": 4.410116993097968e-05,
      "loss": 1.74,
      "step": 1095
    },
    {
      "epoch": 1.76,
      "grad_norm": 2.8936004638671875,
      "learning_rate": 4.363912453058589e-05,
      "loss": 1.745,
      "step": 1100
    },
    {
      "epoch": 1.768,
      "grad_norm": 2.6524393558502197,
      "learning_rate": 4.317763060203664e-05,
      "loss": 1.7675,
      "step": 1105
    },
    {
      "epoch": 1.776,
      "grad_norm": 3.016901731491089,
      "learning_rate": 4.271672815569047e-05,
      "loss": 1.8269,
      "step": 1110
    },
    {
      "epoch": 1.784,
      "grad_norm": 2.8154680728912354,
      "learning_rate": 4.225645715062585e-05,
      "loss": 1.7968,
      "step": 1115
    },
    {
      "epoch": 1.792,
      "grad_norm": 2.8410167694091797,
      "learning_rate": 4.179685749117697e-05,
      "loss": 1.8656,
      "step": 1120
    },
    {
      "epoch": 1.8,
      "grad_norm": 2.7681262493133545,
      "learning_rate": 4.1337969023473964e-05,
      "loss": 1.8008,
      "step": 1125
    },
    {
      "epoch": 1.808,
      "grad_norm": 2.5757415294647217,
      "learning_rate": 4.087983153198848e-05,
      "loss": 1.7254,
      "step": 1130
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 2.874986171722412,
      "learning_rate": 4.042248473608442e-05,
      "loss": 1.8859,
      "step": 1135
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 2.5360043048858643,
      "learning_rate": 3.9965968286574376e-05,
      "loss": 1.7801,
      "step": 1140
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 2.6546435356140137,
      "learning_rate": 3.951032176228199e-05,
      "loss": 1.7976,
      "step": 1145
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 2.858297109603882,
      "learning_rate": 3.90555846666106e-05,
      "loss": 1.7563,
      "step": 1150
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 2.941075325012207,
      "learning_rate": 3.860179642411838e-05,
      "loss": 1.6956,
      "step": 1155
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 2.81112003326416,
      "learning_rate": 3.814899637710031e-05,
      "loss": 1.6929,
      "step": 1160
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 3.150071620941162,
      "learning_rate": 3.769722378217731e-05,
      "loss": 1.7466,
      "step": 1165
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 3.1191582679748535,
      "learning_rate": 3.724651780689286e-05,
      "loss": 1.7693,
      "step": 1170
    },
    {
      "epoch": 1.88,
      "grad_norm": 2.718569278717041,
      "learning_rate": 3.6796917526317156e-05,
      "loss": 1.6758,
      "step": 1175
    },
    {
      "epoch": 1.888,
      "grad_norm": 3.0704891681671143,
      "learning_rate": 3.6348461919659435e-05,
      "loss": 1.8154,
      "step": 1180
    },
    {
      "epoch": 1.896,
      "grad_norm": 2.8869469165802,
      "learning_rate": 3.590118986688865e-05,
      "loss": 1.7672,
      "step": 1185
    },
    {
      "epoch": 1.904,
      "grad_norm": 2.9128432273864746,
      "learning_rate": 3.5455140145362585e-05,
      "loss": 1.7254,
      "step": 1190
    },
    {
      "epoch": 1.912,
      "grad_norm": 3.1494529247283936,
      "learning_rate": 3.5010351426466004e-05,
      "loss": 1.8405,
      "step": 1195
    },
    {
      "epoch": 1.92,
      "grad_norm": 2.81075382232666,
      "learning_rate": 3.456686227225792e-05,
      "loss": 1.7975,
      "step": 1200
    },
    {
      "epoch": 1.928,
      "grad_norm": 2.8771984577178955,
      "learning_rate": 3.412471113212837e-05,
      "loss": 1.6897,
      "step": 1205
    },
    {
      "epoch": 1.936,
      "grad_norm": 2.8166284561157227,
      "learning_rate": 3.368393633946496e-05,
      "loss": 1.7808,
      "step": 1210
    },
    {
      "epoch": 1.944,
      "grad_norm": 2.8906214237213135,
      "learning_rate": 3.324457610832942e-05,
      "loss": 1.7513,
      "step": 1215
    },
    {
      "epoch": 1.952,
      "grad_norm": 3.0797271728515625,
      "learning_rate": 3.280666853014457e-05,
      "loss": 1.7621,
      "step": 1220
    },
    {
      "epoch": 1.96,
      "grad_norm": 2.8338398933410645,
      "learning_rate": 3.237025157039193e-05,
      "loss": 1.7451,
      "step": 1225
    },
    {
      "epoch": 1.968,
      "grad_norm": 2.8984782695770264,
      "learning_rate": 3.193536306532013e-05,
      "loss": 1.7596,
      "step": 1230
    },
    {
      "epoch": 1.976,
      "grad_norm": 2.7546494007110596,
      "learning_rate": 3.150204071866464e-05,
      "loss": 1.776,
      "step": 1235
    },
    {
      "epoch": 1.984,
      "grad_norm": 2.7551515102386475,
      "learning_rate": 3.1070322098378926e-05,
      "loss": 1.5948,
      "step": 1240
    },
    {
      "epoch": 1.992,
      "grad_norm": 3.1570398807525635,
      "learning_rate": 3.064024463337747e-05,
      "loss": 1.8246,
      "step": 1245
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.9426755905151367,
      "learning_rate": 3.0211845610290712e-05,
      "loss": 1.7733,
      "step": 1250
    },
    {
      "epoch": 2.008,
      "grad_norm": 2.841712236404419,
      "learning_rate": 2.9785162170232426e-05,
      "loss": 1.66,
      "step": 1255
    },
    {
      "epoch": 2.016,
      "grad_norm": 2.9757187366485596,
      "learning_rate": 2.9360231305579643e-05,
      "loss": 1.6847,
      "step": 1260
    },
    {
      "epoch": 2.024,
      "grad_norm": 3.1094858646392822,
      "learning_rate": 2.8937089856765564e-05,
      "loss": 1.7093,
      "step": 1265
    },
    {
      "epoch": 2.032,
      "grad_norm": 3.058790445327759,
      "learning_rate": 2.8515774509085535e-05,
      "loss": 1.6462,
      "step": 1270
    },
    {
      "epoch": 2.04,
      "grad_norm": 2.906034469604492,
      "learning_rate": 2.8096321789516555e-05,
      "loss": 1.6827,
      "step": 1275
    },
    {
      "epoch": 2.048,
      "grad_norm": 2.431382656097412,
      "learning_rate": 2.7678768063550452e-05,
      "loss": 1.6446,
      "step": 1280
    },
    {
      "epoch": 2.056,
      "grad_norm": 2.7515854835510254,
      "learning_rate": 2.726314953204111e-05,
      "loss": 1.7441,
      "step": 1285
    },
    {
      "epoch": 2.064,
      "grad_norm": 3.0361056327819824,
      "learning_rate": 2.684950222806596e-05,
      "loss": 1.7794,
      "step": 1290
    },
    {
      "epoch": 2.072,
      "grad_norm": 2.8051040172576904,
      "learning_rate": 2.643786201380194e-05,
      "loss": 1.6431,
      "step": 1295
    },
    {
      "epoch": 2.08,
      "grad_norm": 3.0298259258270264,
      "learning_rate": 2.602826457741642e-05,
      "loss": 1.6764,
      "step": 1300
    },
    {
      "epoch": 2.088,
      "grad_norm": 2.7564265727996826,
      "learning_rate": 2.5620745429973047e-05,
      "loss": 1.7067,
      "step": 1305
    },
    {
      "epoch": 2.096,
      "grad_norm": 3.1632637977600098,
      "learning_rate": 2.5215339902353098e-05,
      "loss": 1.7141,
      "step": 1310
    },
    {
      "epoch": 2.104,
      "grad_norm": 3.3215579986572266,
      "learning_rate": 2.4812083142192328e-05,
      "loss": 1.661,
      "step": 1315
    },
    {
      "epoch": 2.112,
      "grad_norm": 2.931262969970703,
      "learning_rate": 2.441101011083378e-05,
      "loss": 1.6721,
      "step": 1320
    },
    {
      "epoch": 2.12,
      "grad_norm": 3.2005648612976074,
      "learning_rate": 2.401215558029671e-05,
      "loss": 1.5691,
      "step": 1325
    },
    {
      "epoch": 2.128,
      "grad_norm": 2.9086577892303467,
      "learning_rate": 2.3615554130262e-05,
      "loss": 1.6601,
      "step": 1330
    },
    {
      "epoch": 2.136,
      "grad_norm": 3.094104766845703,
      "learning_rate": 2.3221240145074098e-05,
      "loss": 1.7662,
      "step": 1335
    },
    {
      "epoch": 2.144,
      "grad_norm": 3.1231937408447266,
      "learning_rate": 2.2829247810760025e-05,
      "loss": 1.6892,
      "step": 1340
    },
    {
      "epoch": 2.152,
      "grad_norm": 3.1029539108276367,
      "learning_rate": 2.243961111206555e-05,
      "loss": 1.6883,
      "step": 1345
    },
    {
      "epoch": 2.16,
      "grad_norm": 2.921509027481079,
      "learning_rate": 2.2052363829508775e-05,
      "loss": 1.6201,
      "step": 1350
    },
    {
      "epoch": 2.168,
      "grad_norm": 3.1711323261260986,
      "learning_rate": 2.1667539536451453e-05,
      "loss": 1.7186,
      "step": 1355
    },
    {
      "epoch": 2.176,
      "grad_norm": 3.0876011848449707,
      "learning_rate": 2.1285171596188268e-05,
      "loss": 1.7158,
      "step": 1360
    },
    {
      "epoch": 2.184,
      "grad_norm": 3.036311149597168,
      "learning_rate": 2.090529315905431e-05,
      "loss": 1.6053,
      "step": 1365
    },
    {
      "epoch": 2.192,
      "grad_norm": 2.9391181468963623,
      "learning_rate": 2.052793715955104e-05,
      "loss": 1.7479,
      "step": 1370
    },
    {
      "epoch": 2.2,
      "grad_norm": 3.1582813262939453,
      "learning_rate": 2.0153136313490943e-05,
      "loss": 1.6349,
      "step": 1375
    },
    {
      "epoch": 2.208,
      "grad_norm": 3.2014708518981934,
      "learning_rate": 1.9780923115161158e-05,
      "loss": 1.7643,
      "step": 1380
    },
    {
      "epoch": 2.216,
      "grad_norm": 2.639417886734009,
      "learning_rate": 1.9411329834506286e-05,
      "loss": 1.647,
      "step": 1385
    },
    {
      "epoch": 2.224,
      "grad_norm": 3.020808458328247,
      "learning_rate": 1.904438851433068e-05,
      "loss": 1.6426,
      "step": 1390
    },
    {
      "epoch": 2.232,
      "grad_norm": 3.0100085735321045,
      "learning_rate": 1.8680130967520433e-05,
      "loss": 1.5914,
      "step": 1395
    },
    {
      "epoch": 2.24,
      "grad_norm": 3.194267749786377,
      "learning_rate": 1.831858877428524e-05,
      "loss": 1.6745,
      "step": 1400
    },
    {
      "epoch": 2.248,
      "grad_norm": 3.05558443069458,
      "learning_rate": 1.7959793279420507e-05,
      "loss": 1.6198,
      "step": 1405
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 2.9922232627868652,
      "learning_rate": 1.7603775589589823e-05,
      "loss": 1.6549,
      "step": 1410
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 2.8694519996643066,
      "learning_rate": 1.7250566570628103e-05,
      "loss": 1.6306,
      "step": 1415
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 3.0081636905670166,
      "learning_rate": 1.6900196844865573e-05,
      "loss": 1.6746,
      "step": 1420
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 2.885333776473999,
      "learning_rate": 1.6552696788472922e-05,
      "loss": 1.6208,
      "step": 1425
    },
    {
      "epoch": 2.288,
      "grad_norm": 3.3863255977630615,
      "learning_rate": 1.6208096528827715e-05,
      "loss": 1.6302,
      "step": 1430
    },
    {
      "epoch": 2.296,
      "grad_norm": 3.113079071044922,
      "learning_rate": 1.5866425941902524e-05,
      "loss": 1.6933,
      "step": 1435
    },
    {
      "epoch": 2.304,
      "grad_norm": 2.9286105632781982,
      "learning_rate": 1.5527714649674642e-05,
      "loss": 1.6496,
      "step": 1440
    },
    {
      "epoch": 2.312,
      "grad_norm": 3.209777355194092,
      "learning_rate": 1.5191992017557993e-05,
      "loss": 1.7381,
      "step": 1445
    },
    {
      "epoch": 2.32,
      "grad_norm": 3.198620319366455,
      "learning_rate": 1.485928715185721e-05,
      "loss": 1.6181,
      "step": 1450
    },
    {
      "epoch": 2.328,
      "grad_norm": 2.9119386672973633,
      "learning_rate": 1.4529628897244212e-05,
      "loss": 1.678,
      "step": 1455
    },
    {
      "epoch": 2.336,
      "grad_norm": 3.6534738540649414,
      "learning_rate": 1.4203045834257416e-05,
      "loss": 1.6278,
      "step": 1460
    },
    {
      "epoch": 2.344,
      "grad_norm": 3.0995583534240723,
      "learning_rate": 1.3879566276823896e-05,
      "loss": 1.7287,
      "step": 1465
    },
    {
      "epoch": 2.352,
      "grad_norm": 3.0108957290649414,
      "learning_rate": 1.3559218269804625e-05,
      "loss": 1.7037,
      "step": 1470
    },
    {
      "epoch": 2.36,
      "grad_norm": 2.899688482284546,
      "learning_rate": 1.3242029586563054e-05,
      "loss": 1.704,
      "step": 1475
    },
    {
      "epoch": 2.368,
      "grad_norm": 3.0882275104522705,
      "learning_rate": 1.2928027726557257e-05,
      "loss": 1.6781,
      "step": 1480
    },
    {
      "epoch": 2.376,
      "grad_norm": 2.990762710571289,
      "learning_rate": 1.2617239912955758e-05,
      "loss": 1.6697,
      "step": 1485
    },
    {
      "epoch": 2.384,
      "grad_norm": 3.097442626953125,
      "learning_rate": 1.230969309027739e-05,
      "loss": 1.6723,
      "step": 1490
    },
    {
      "epoch": 2.392,
      "grad_norm": 3.3563499450683594,
      "learning_rate": 1.2005413922055247e-05,
      "loss": 1.6338,
      "step": 1495
    },
    {
      "epoch": 2.4,
      "grad_norm": 2.9378817081451416,
      "learning_rate": 1.170442878852503e-05,
      "loss": 1.6456,
      "step": 1500
    },
    {
      "epoch": 2.408,
      "grad_norm": 3.0075063705444336,
      "learning_rate": 1.1406763784337948e-05,
      "loss": 1.6965,
      "step": 1505
    },
    {
      "epoch": 2.416,
      "grad_norm": 3.088571310043335,
      "learning_rate": 1.1112444716298381e-05,
      "loss": 1.656,
      "step": 1510
    },
    {
      "epoch": 2.424,
      "grad_norm": 3.1936097145080566,
      "learning_rate": 1.0821497101126487e-05,
      "loss": 1.768,
      "step": 1515
    },
    {
      "epoch": 2.432,
      "grad_norm": 3.3181843757629395,
      "learning_rate": 1.0533946163245983e-05,
      "loss": 1.6903,
      "step": 1520
    },
    {
      "epoch": 2.44,
      "grad_norm": 3.268173933029175,
      "learning_rate": 1.024981683259723e-05,
      "loss": 1.7462,
      "step": 1525
    },
    {
      "epoch": 2.448,
      "grad_norm": 2.718535900115967,
      "learning_rate": 9.969133742475884e-06,
      "loss": 1.6714,
      "step": 1530
    },
    {
      "epoch": 2.456,
      "grad_norm": 2.9967477321624756,
      "learning_rate": 9.691921227397227e-06,
      "loss": 1.7305,
      "step": 1535
    },
    {
      "epoch": 2.464,
      "grad_norm": 3.3666961193084717,
      "learning_rate": 9.418203320986502e-06,
      "loss": 1.6929,
      "step": 1540
    },
    {
      "epoch": 2.472,
      "grad_norm": 2.983024835586548,
      "learning_rate": 9.148003753895145e-06,
      "loss": 1.7467,
      "step": 1545
    },
    {
      "epoch": 2.48,
      "grad_norm": 2.887960910797119,
      "learning_rate": 8.881345951743486e-06,
      "loss": 1.7202,
      "step": 1550
    },
    {
      "epoch": 2.488,
      "grad_norm": 2.9530959129333496,
      "learning_rate": 8.618253033089769e-06,
      "loss": 1.6711,
      "step": 1555
    },
    {
      "epoch": 2.496,
      "grad_norm": 3.1805944442749023,
      "learning_rate": 8.358747807425826e-06,
      "loss": 1.6889,
      "step": 1560
    },
    {
      "epoch": 2.504,
      "grad_norm": 3.112802743911743,
      "learning_rate": 8.102852773199588e-06,
      "loss": 1.7712,
      "step": 1565
    },
    {
      "epoch": 2.512,
      "grad_norm": 3.2490224838256836,
      "learning_rate": 7.850590115864481e-06,
      "loss": 1.7063,
      "step": 1570
    },
    {
      "epoch": 2.52,
      "grad_norm": 2.898257255554199,
      "learning_rate": 7.60198170595604e-06,
      "loss": 1.6384,
      "step": 1575
    },
    {
      "epoch": 2.528,
      "grad_norm": 2.834970235824585,
      "learning_rate": 7.357049097195773e-06,
      "loss": 1.7293,
      "step": 1580
    },
    {
      "epoch": 2.536,
      "grad_norm": 3.1402721405029297,
      "learning_rate": 7.115813524622489e-06,
      "loss": 1.7956,
      "step": 1585
    },
    {
      "epoch": 2.544,
      "grad_norm": 3.0107924938201904,
      "learning_rate": 6.8782959027513195e-06,
      "loss": 1.6945,
      "step": 1590
    },
    {
      "epoch": 2.552,
      "grad_norm": 2.8891756534576416,
      "learning_rate": 6.6445168237604385e-06,
      "loss": 1.7376,
      "step": 1595
    },
    {
      "epoch": 2.56,
      "grad_norm": 2.894723892211914,
      "learning_rate": 6.414496555705801e-06,
      "loss": 1.636,
      "step": 1600
    },
    {
      "epoch": 2.568,
      "grad_norm": 3.2135400772094727,
      "learning_rate": 6.18825504076393e-06,
      "loss": 1.6455,
      "step": 1605
    },
    {
      "epoch": 2.576,
      "grad_norm": 3.1758835315704346,
      "learning_rate": 5.965811893503015e-06,
      "loss": 1.736,
      "step": 1610
    },
    {
      "epoch": 2.584,
      "grad_norm": 2.939340353012085,
      "learning_rate": 5.747186399182336e-06,
      "loss": 1.6702,
      "step": 1615
    },
    {
      "epoch": 2.592,
      "grad_norm": 3.0350241661071777,
      "learning_rate": 5.532397512080306e-06,
      "loss": 1.6339,
      "step": 1620
    },
    {
      "epoch": 2.6,
      "grad_norm": 3.2611632347106934,
      "learning_rate": 5.321463853851188e-06,
      "loss": 1.6075,
      "step": 1625
    },
    {
      "epoch": 2.608,
      "grad_norm": 3.249307632446289,
      "learning_rate": 5.114403711910632e-06,
      "loss": 1.5996,
      "step": 1630
    },
    {
      "epoch": 2.616,
      "grad_norm": 3.306370258331299,
      "learning_rate": 4.911235037850187e-06,
      "loss": 1.7061,
      "step": 1635
    },
    {
      "epoch": 2.624,
      "grad_norm": 2.8737359046936035,
      "learning_rate": 4.711975445880972e-06,
      "loss": 1.7054,
      "step": 1640
    },
    {
      "epoch": 2.632,
      "grad_norm": 3.1082751750946045,
      "learning_rate": 4.516642211306588e-06,
      "loss": 1.6182,
      "step": 1645
    },
    {
      "epoch": 2.64,
      "grad_norm": 3.136613130569458,
      "learning_rate": 4.325252269025315e-06,
      "loss": 1.6595,
      "step": 1650
    },
    {
      "epoch": 2.648,
      "grad_norm": 3.0927417278289795,
      "learning_rate": 4.137822212061965e-06,
      "loss": 1.7556,
      "step": 1655
    },
    {
      "epoch": 2.656,
      "grad_norm": 3.165540933609009,
      "learning_rate": 3.954368290129301e-06,
      "loss": 1.5752,
      "step": 1660
    },
    {
      "epoch": 2.664,
      "grad_norm": 3.018314838409424,
      "learning_rate": 3.7749064082191977e-06,
      "loss": 1.5969,
      "step": 1665
    },
    {
      "epoch": 2.672,
      "grad_norm": 3.2570745944976807,
      "learning_rate": 3.5994521252237513e-06,
      "loss": 1.6771,
      "step": 1670
    },
    {
      "epoch": 2.68,
      "grad_norm": 3.1760969161987305,
      "learning_rate": 3.42802065258635e-06,
      "loss": 1.5805,
      "step": 1675
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 3.2157087326049805,
      "learning_rate": 3.260626852982873e-06,
      "loss": 1.6809,
      "step": 1680
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 2.9363861083984375,
      "learning_rate": 3.0972852390331375e-06,
      "loss": 1.6563,
      "step": 1685
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 3.096524953842163,
      "learning_rate": 2.93800997204271e-06,
      "loss": 1.7144,
      "step": 1690
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 3.0993595123291016,
      "learning_rate": 2.7828148607751236e-06,
      "loss": 1.6597,
      "step": 1695
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 3.0494582653045654,
      "learning_rate": 2.631713360254734e-06,
      "loss": 1.681,
      "step": 1700
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 3.059680461883545,
      "learning_rate": 2.4847185706001643e-06,
      "loss": 1.7494,
      "step": 1705
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 3.117607593536377,
      "learning_rate": 2.3418432358885634e-06,
      "loss": 1.8558,
      "step": 1710
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 3.0401954650878906,
      "learning_rate": 2.203099743050746e-06,
      "loss": 1.7049,
      "step": 1715
    },
    {
      "epoch": 2.752,
      "grad_norm": 3.139512777328491,
      "learning_rate": 2.0685001207972844e-06,
      "loss": 1.6446,
      "step": 1720
    },
    {
      "epoch": 2.76,
      "grad_norm": 3.145209312438965,
      "learning_rate": 1.9380560385756086e-06,
      "loss": 1.7555,
      "step": 1725
    },
    {
      "epoch": 2.768,
      "grad_norm": 3.2214560508728027,
      "learning_rate": 1.8117788055583284e-06,
      "loss": 1.8187,
      "step": 1730
    },
    {
      "epoch": 2.776,
      "grad_norm": 3.068183183670044,
      "learning_rate": 1.68967936966275e-06,
      "loss": 1.7019,
      "step": 1735
    },
    {
      "epoch": 2.784,
      "grad_norm": 3.302934169769287,
      "learning_rate": 1.5717683166017182e-06,
      "loss": 1.751,
      "step": 1740
    },
    {
      "epoch": 2.792,
      "grad_norm": 2.951707363128662,
      "learning_rate": 1.4580558689658408e-06,
      "loss": 1.6534,
      "step": 1745
    },
    {
      "epoch": 2.8,
      "grad_norm": 3.2267186641693115,
      "learning_rate": 1.3485518853372624e-06,
      "loss": 1.7512,
      "step": 1750
    },
    {
      "epoch": 2.808,
      "grad_norm": 3.200103998184204,
      "learning_rate": 1.2432658594349111e-06,
      "loss": 1.7179,
      "step": 1755
    },
    {
      "epoch": 2.816,
      "grad_norm": 3.014716148376465,
      "learning_rate": 1.142206919291422e-06,
      "loss": 1.7531,
      "step": 1760
    },
    {
      "epoch": 2.824,
      "grad_norm": 2.9868645668029785,
      "learning_rate": 1.045383826461771e-06,
      "loss": 1.7479,
      "step": 1765
    },
    {
      "epoch": 2.832,
      "grad_norm": 2.9439401626586914,
      "learning_rate": 9.528049752636714e-07,
      "loss": 1.7236,
      "step": 1770
    },
    {
      "epoch": 2.84,
      "grad_norm": 3.1172292232513428,
      "learning_rate": 8.644783920498e-07,
      "loss": 1.6575,
      "step": 1775
    },
    {
      "epoch": 2.848,
      "grad_norm": 2.8912501335144043,
      "learning_rate": 7.804117345119266e-07,
      "loss": 1.6974,
      "step": 1780
    },
    {
      "epoch": 2.856,
      "grad_norm": 3.172733783721924,
      "learning_rate": 7.006122910170221e-07,
      "loss": 1.6505,
      "step": 1785
    },
    {
      "epoch": 2.864,
      "grad_norm": 3.143495559692383,
      "learning_rate": 6.250869799753866e-07,
      "loss": 1.6933,
      "step": 1790
    },
    {
      "epoch": 2.872,
      "grad_norm": 2.984405755996704,
      "learning_rate": 5.538423492408129e-07,
      "loss": 1.6931,
      "step": 1795
    },
    {
      "epoch": 2.88,
      "grad_norm": 3.0550923347473145,
      "learning_rate": 4.868845755429174e-07,
      "loss": 1.7448,
      "step": 1800
    },
    {
      "epoch": 2.888,
      "grad_norm": 3.081655979156494,
      "learning_rate": 4.242194639516417e-07,
      "loss": 1.7102,
      "step": 1805
    },
    {
      "epoch": 2.896,
      "grad_norm": 2.9326679706573486,
      "learning_rate": 3.658524473739544e-07,
      "loss": 1.639,
      "step": 1810
    },
    {
      "epoch": 2.904,
      "grad_norm": 3.317094087600708,
      "learning_rate": 3.117885860828396e-07,
      "loss": 1.6976,
      "step": 1815
    },
    {
      "epoch": 2.912,
      "grad_norm": 3.1869325637817383,
      "learning_rate": 2.620325672785917e-07,
      "loss": 1.6893,
      "step": 1820
    },
    {
      "epoch": 2.92,
      "grad_norm": 3.205848455429077,
      "learning_rate": 2.1658870468241333e-07,
      "loss": 1.6767,
      "step": 1825
    },
    {
      "epoch": 2.928,
      "grad_norm": 3.0166637897491455,
      "learning_rate": 1.7546093816246388e-07,
      "loss": 1.6307,
      "step": 1830
    },
    {
      "epoch": 2.936,
      "grad_norm": 2.8889715671539307,
      "learning_rate": 1.3865283339228319e-07,
      "loss": 1.4917,
      "step": 1835
    },
    {
      "epoch": 2.944,
      "grad_norm": 3.251251697540283,
      "learning_rate": 1.0616758154161632e-07,
      "loss": 1.7014,
      "step": 1840
    },
    {
      "epoch": 2.952,
      "grad_norm": 2.8703055381774902,
      "learning_rate": 7.800799899979061e-08,
      "loss": 1.6134,
      "step": 1845
    },
    {
      "epoch": 2.96,
      "grad_norm": 2.870368480682373,
      "learning_rate": 5.417652713152199e-08,
      "loss": 1.6504,
      "step": 1850
    },
    {
      "epoch": 2.968,
      "grad_norm": 3.110130548477173,
      "learning_rate": 3.467523206525658e-08,
      "loss": 1.7378,
      "step": 1855
    },
    {
      "epoch": 2.976,
      "grad_norm": 2.866793394088745,
      "learning_rate": 1.9505804514047266e-08,
      "loss": 1.6231,
      "step": 1860
    },
    {
      "epoch": 2.984,
      "grad_norm": 3.036414623260498,
      "learning_rate": 8.669559628954327e-09,
      "loss": 1.6624,
      "step": 1865
    },
    {
      "epoch": 2.992,
      "grad_norm": 3.3270976543426514,
      "learning_rate": 2.167436885064378e-09,
      "loss": 1.698,
      "step": 1870
    },
    {
      "epoch": 3.0,
      "grad_norm": 3.082894802093506,
      "learning_rate": 0.0,
      "loss": 1.6867,
      "step": 1875
    }
  ],
  "logging_steps": 5,
  "max_steps": 1875,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.458660762415104e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
